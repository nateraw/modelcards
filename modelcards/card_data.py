from dataclasses import asdict, dataclass
from typing import Any, Dict, List, Optional, Union

import yaml


@dataclass
class EvalResult:
    """Flattened representation of individual evaluation results found in model-index."""

    # Required

    # The task identifier
    # Example: automatic-speech-recognition
    task_type: str

    # The dataset identifier
    # Example: common_voice. Use dataset id from https://hf.co/datasets
    dataset_type: str

    # A pretty name for the dataset.
    # Example: Common Voice (French)
    dataset_name: str

    # The metric identifier
    # Example: wer. Use metric id from https://hf.co/metrics
    metric_type: str

    # Value of the metric.
    # Example: 20.0 or "20.0 Â± 1.2"
    metric_value: Any

    # Optional

    # A pretty name for the task.
    # Example: Speech Recognition
    task_name: Optional[str] = None

    # The name of the dataset configuration used in `load_dataset()`.
    # Example: fr in `load_dataset("common_voice", "fr")`.
    # See the `datasets` docs for more info:
    # https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.load_dataset.name
    dataset_config: Optional[str] = None

    # The split used in `load_dataset()`.
    # Example: test
    dataset_split: Optional[str] = None

    # The revision (AKA Git Sha) of the dataset used in `load_dataset()`.
    # Example: 5503434ddd753f426f4b38109466949a1217c2bb
    dataset_revision: Optional[str] = None

    # The arguments passed during `Metric.compute()`.
    # Example for `bleu`: max_order: 4
    dataset_args: Optional[Dict[str, Any]] = None

    # A pretty name for the metric.
    # Example: Test WER
    metric_name: Optional[str] = None

    # The name of the metric configuration used in `load_metric()`.
    # Example: bleurt-large-512 in `load_metric("bleurt", "bleurt-large-512")`.
    # See the `datasets` docs for more info: https://huggingface.co/docs/datasets/v2.1.0/en/loading#load-configurations
    metric_config: Optional[str] = None

    # The arguments passed during `Metric.compute()`.
    # Example for `bleu`: max_order: 4
    metric_args: Optional[Dict[str, Any]] = None

    # If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).
    verified: Optional[bool] = None


@dataclass
class CardData:
    """Model Card Metadata that is used by Hugging Face Hub when included at the top of your README.md"""

    language: Optional[Union[str, List[str]]] = None
    license: Optional[str] = None
    library_name: Optional[str] = None
    tags: Optional[List[str]] = None
    datasets: Optional[Union[str, List[str]]] = None
    metrics: Optional[Union[str, List[str]]] = None
    eval_results: Optional[List[EvalResult]] = None
    model_name: Optional[str] = None

    def __post_init__(self):
        if self.eval_results:
            if type(self.eval_results) == EvalResult:
                self.eval_results = [self.eval_results]
            if self.model_name is None:
                raise ValueError("`eval_results` requires `model_name` to be set.")

    def to_dict(self):

        data_dict = asdict(self)
        if self.eval_results is not None:
            data_dict["model-index"] = eval_results_to_model_index(
                self.model_name, self.eval_results
            )
            del data_dict["eval_results"], data_dict["model_name"]

        return _remove_none(data_dict)

    def to_yaml(self):
        return yaml.dump(self.to_dict(), sort_keys=False).strip()


def model_index_to_eval_results(model_index):
    """Takes in a model index and returns a list of `modelcards.EvalResult` objects."""
    eval_results = []
    for elem in model_index:
        name = elem["name"]
        results = elem["results"]
        for result in results:
            task_type = result["task"]["type"]
            task_name = result["task"].get("name")
            dataset_type = result["dataset"]["type"]
            dataset_name = result["dataset"]["name"]
            dataset_config = result["dataset"].get("config")
            dataset_split = result["dataset"].get("split")
            dataset_revision = result["dataset"].get("revision")
            dataset_args = result["dataset"].get("args")

            for metric in result["metrics"]:
                metric_type = metric["type"]
                metric_value = metric["value"]
                metric_name = metric.get("name")
                metric_args = metric.get("args")
                verified = metric.get("verified")

                eval_result = EvalResult(
                    task_type=task_type,  # Required
                    dataset_type=dataset_type,  # Required
                    dataset_name=dataset_name,  # Required
                    metric_type=metric_type,  # Required
                    metric_value=metric_value,  # Required
                    task_name=task_name,
                    dataset_config=dataset_config,
                    dataset_split=dataset_split,
                    dataset_revision=dataset_revision,
                    dataset_args=dataset_args,
                    metric_name=metric_name,
                    metric_args=metric_args,
                    verified=verified,
                )
                eval_results.append(eval_result)
    return name, eval_results


def _remove_none(obj):
    """Recursively remove `None` values from a dict. Borrowed from: https://stackoverflow.com/a/20558778"""
    if isinstance(obj, (list, tuple, set)):
        return type(obj)(_remove_none(x) for x in obj if x is not None)
    elif isinstance(obj, dict):
        return type(obj)(
            (_remove_none(k), _remove_none(v))
            for k, v in obj.items()
            if k is not None and v is not None
        )
    else:
        return obj


def eval_results_to_model_index(model_name: str, eval_results: List[EvalResult]):
    """Takes in given model name and list of `modelcards.EvalResult` and returns a valid model-index"""
    task_and_ds_types_map = dict()
    for eval_result in eval_results:
        task_and_ds_pair = (eval_result.task_type, eval_result.dataset_type)
        if task_and_ds_pair in task_and_ds_types_map:
            task_and_ds_types_map[task_and_ds_pair].append(eval_result)
        else:
            task_and_ds_types_map[task_and_ds_pair] = [eval_result]

    model_index_data = []
    for (task_type, dataset_type), results in task_and_ds_types_map.items():
        data = {
            "task": {
                "type": task_type,
                "name": results[0].task_name,
            },
            "dataset": {
                "type": dataset_type,
                "name": results[0].dataset_name,
                "config": results[0].dataset_config,
                "split": results[0].dataset_split,
                "revision": results[0].dataset_revision,
                "args": results[0].dataset_args,
            },
            "metrics": [
                {
                    "name": result.metric_name,
                    "type": result.metric_type,
                    "value": result.metric_value,
                    "args": result.metric_args,
                    "verified": result.verified,
                }
                for result in results
            ],
        }
        model_index_data.append(data)

    model_index = [
        {
            "name": model_name,
            "results": model_index_data,
        }
    ]
    return _remove_none(model_index)
