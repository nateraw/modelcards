from dataclasses import asdict, dataclass
from typing import Any, Dict, List, Optional, Union

import yaml


@dataclass
class EvalResult:

    # Required

    # The model name.
    # Example: If your repo ID is "nateraw/cool-model", then your model name is "cool-model".
    # Example: "resnet50-imagenet-pretrained"
    name: str

    # The task identifier
    # Example: automatic-speech-recognition
    task_type: str

    # The dataset identifier
    # Example: common_voice. Use dataset id from https://hf.co/datasets
    dataset_type: str

    # A pretty name for the dataset.
    # Example: Common Voice (French)
    dataset_name: str

    # The metric identifier
    # Example: wer. Use metric id from https://hf.co/metrics
    metric_type: str

    # Value of the metric.
    # Example: 20.0 or "20.0 Â± 1.2"
    metric_value: Any

    # Optional

    # A pretty name for the task.
    # Example: Speech Recognition
    task_name: Optional[str] = None

    # The name of the dataset configuration used in `load_dataset()`.
    # Example: fr in `load_dataset("common_voice", "fr")`.
    # See the `datasets` docs for more info:
    # https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.load_dataset.name
    dataset_config: Optional[str] = None

    # The split used in `load_dataset()`.
    # Example: test
    dataset_split: Optional[str] = None

    # The revision (AKA Git Sha) of the dataset used in `load_dataset()`.
    # Example: 5503434ddd753f426f4b38109466949a1217c2bb
    dataset_revision: Optional[str] = None

    # The arguments passed during `Metric.compute()`.
    # Example for `bleu`: max_order: 4
    dataset_args: Optional[Dict[str, Any]] = None

    # A pretty name for the metric.
    # Example: Test WER
    metric_name: Optional[str] = None

    # The name of the metric configuration used in `load_metric()`.
    # Example: bleurt-large-512 in `load_metric("bleurt", "bleurt-large-512")`.
    # See the `datasets` docs for more info: https://huggingface.co/docs/datasets/v2.1.0/en/loading#load-configurations
    metric_config: Optional[str] = None

    # The arguments passed during `Metric.compute()`.
    # Example for `bleu`: max_order: 4
    metric_args: Optional[Dict[str, Any]] = None

    # If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).
    verified: Optional[bool] = None


@dataclass
class CardData:
    language: Optional[Union[str, List[str]]] = None
    license: Optional[str] = None
    library_name: Optional[str] = None
    tags: Optional[List[str]] = None
    datasets: Optional[Union[str, List[str]]] = None
    metrics: Optional[Union[str, List[str]]] = None
    eval_results: Optional[List[EvalResult]] = None

    def to_dict(self):

        data_dict = asdict(self)
        if self.eval_results is not None:
            data_dict['model-index'] = eval_results_to_model_index(self.eval_results)
            del data_dict['eval_results']

        return _remove_none(data_dict)

    def to_yaml(self):
        return yaml.dump(self.to_dict(), sort_keys=False).strip()


def model_index_to_eval_results(model_index):
    eval_results = []
    for elem in model_index:
        name = elem['name']
        results = elem['results']
        for result in results:
            task_type = result['task']['type']
            task_name = result['task'].get('name')
            dataset_type = result['dataset']['type']
            dataset_name = result['dataset']['name']
            dataset_config = result['dataset'].get('config')
            dataset_split = result['dataset'].get('split')
            dataset_revision = result['dataset'].get('revision')
            dataset_args = result['dataset'].get('args')

            for metric in result['metrics']:
                metric_type = metric['type']
                metric_value = metric['value']
                metric_name = metric.get('name')
                metric_args = metric.get('args')
                verified = metric.get('verified')

                eval_result = EvalResult(
                    name=name,  # Required
                    task_type=task_type,  # Required
                    dataset_type=dataset_type,  # Required
                    dataset_name=dataset_name,  # Required
                    metric_type=metric_type,  # Required
                    metric_value=metric_value,  # Required
                    task_name=task_name,
                    dataset_config=dataset_config,
                    dataset_split=dataset_split,
                    dataset_revision=dataset_revision,
                    dataset_args=dataset_args,
                    metric_name=metric_name,
                    metric_args=metric_args,
                    verified=verified,
                )
                eval_results.append(eval_result)
    return eval_results


def _remove_none(obj):
    """Recursively remove `None` values from a dict. Borrowed from: https://stackoverflow.com/a/20558778"""
    if isinstance(obj, (list, tuple, set)):
        return type(obj)(_remove_none(x) for x in obj if x is not None)
    elif isinstance(obj, dict):
        return type(obj)((_remove_none(k), _remove_none(v)) for k, v in obj.items() if k is not None and v is not None)
    else:
        return obj


def eval_results_to_model_index(eval_results: List[EvalResult]):

    task_and_ds_types_map = dict()
    for eval_result in eval_results:
        task_and_ds_pair = (eval_result.task_type, eval_result.dataset_type)
        if task_and_ds_pair in task_and_ds_types_map:
            task_and_ds_types_map[task_and_ds_pair].append(eval_result)
        else:
            task_and_ds_types_map[task_and_ds_pair] = [eval_result]

    model_name = eval_results[0].name
    model_index_data = []
    for (task_type, dataset_type), results in task_and_ds_types_map.items():
        data = {
            'task': {
                'type': task_type,
                'name': results[0].task_name,
            },
            'dataset': {
                'type': dataset_type,
                'name': results[0].dataset_name,
                'config': results[0].dataset_config,
                'split': results[0].dataset_split,
                'revision': results[0].dataset_revision,
                'args': results[0].dataset_args,
            },
            'metrics': [
                {
                    'name': result.metric_name,
                    'type': result.metric_type,
                    'value': result.metric_value,
                    'args': result.metric_args,
                    'verified': result.verified,
                }
                for result in results
            ],
        }
        model_index_data.append(data)

    model_index = [
        {
            'name': model_name,
            'results': model_index_data,
        }
    ]
    return _remove_none(model_index)
